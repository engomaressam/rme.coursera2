import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import itertools
import numpy as np

model.eval()
with torch.no_grad():
    y_pred_test_probs = model(X_test)
    y_pred_test_labels = (y_pred_test_probs > 0.5).float()

# Convert tensors to numpy for sklearn metrics
y_test_np = y_test.numpy()
y_pred_test_labels_np = y_pred_test_labels.numpy()
y_pred_test_probs_np = y_pred_test_probs.numpy()

# 1. Visualize the confusion matrix
cm = confusion_matrix(y_test_np, y_pred_test_labels_np)
plt.figure(figsize=(6, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(2)
plt.xticks(tick_marks, ['Loss', 'Win'], rotation=45)
plt.yticks(tick_marks, ['Loss', 'Win'])

thresh = cm.max() / 2
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")

plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

# 2. Print classification report
print("Classification Report:\n", classification_report(y_test_np, y_pred_test_labels_np, target_names=['Loss', 'Win']))

# 3. Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test_np, y_pred_test_probs_np)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()